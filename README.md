# Transformer Encoder â€“ Autoencoding (Masked Language Model)

## Objective
To understand Transformer Encoder architecture, self-attention, and autoencoding
by reconstructing masked text using a Transformer-based model.

## Key Concepts
- Self-Attention
- Positional Encoding
- Masked Language Modeling
- Autoencoding

## Project Structure
- attention.py
- encoder.py
- positional_encoding.py
- train_mlm.py
- visualize_attention.ipynb
- results/
